{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Step 4:Data Collection \n",
    "\n",
    "## Scraping web pages for food recipes using Scrapy\n",
    "The Python library Scrapy is used to collect food recipes from specified recipe aggregation\n",
    "websites such as Allrecipes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The crawler\n",
    "The crawler defined below is tailored to navigate the site structure in allrecipes.com and\n",
    "collect recipe information.\n",
    "\n",
    "The entry point for the crawler is the site directory on allrecipes.com, which lists all the recipe topics\n",
    "available on the website. Navigating the hyperlink to each of these topics yields another page with links to\n",
    "individual recipes and possible subcategories. For example, the page for the topic \"breakfast and brunch\"\n",
    "contains links to pancake and french toast recipes, but also includes links to subtopics such as \"breakfast bread\"\n",
    "and \"breakfast eggs\".<br> \n",
    "<br>The crawler is designed to reach the bottommost subtopic before searching for links to food recipes. \n",
    "Once the crawler follows the links from topics to a page with recipe instructions, it will collect three main types of recipe text: \n",
    "recipe name, required ingredients, and instructions.\n",
    "The entire page content in html is also saved for later parsing. These four data points constitute the crawl result for a recipe page, which will\n",
    "all be gathered into a single JSONLines file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import scrapy\n",
    "import os\n",
    "import urllib.parse as url\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.shell import inspect_response\n",
    "\n",
    "\n",
    "class AllRecipesSpider(scrapy.Spider):\n",
    "    name = \"allrecipes\"\n",
    "\n",
    "    @classmethod\n",
    "    def update_settings(cls, settings):\n",
    "        super().update_settings(settings)\n",
    "        # enabled_ext = {\"scrapy.extensions.closespider.CloseSpider\": 100}\n",
    "        # settings.set(\"EXTENSIONS\", {})\n",
    "        settings.set(\"CLOSESPIDER_ITEMCOUNT\", 0, priority=\"spider\")\n",
    "        settings.set(\n",
    "            \"FEEDS\", {\"recipes.jl\": {\"format\": \"jsonlines\", \"overwrite\": True}}\n",
    "        )\n",
    "\n",
    "    def start_requests(self):\n",
    "        url = \"https://www.allrecipes.com/recipes-a-z-6735880\"\n",
    "        yield scrapy.Request(url, self.parse_a_z)\n",
    "\n",
    "    def parse_a_z(self, response):\n",
    "        # Capture all category items in A-Z list\n",
    "        for category in response.xpath(\n",
    "            \"//li[contains(@id,'link-list__item_1-0')]/a[contains(@href,'recipes')]/@href\"\n",
    "        ):\n",
    "            yield scrapy.Request(category.get(), self.parse_categories)\n",
    "\n",
    "    def parse_categories(self, response):\n",
    "        url_path_components = url.urlsplit(response.url)[2].strip(\"/\").split(\"/\")\n",
    "        main_category = url_path_components[-1]\n",
    "        print(main_category)\n",
    "        # Capture all subcategory links\n",
    "        for category in response.xpath(\n",
    "            f\"//a[contains(@href, '{main_category}')]/span[@class='link__wrapper']/parent::*/@href\"\n",
    "        ).getall():\n",
    "            yield scrapy.Request(category, self.parse_categories)\n",
    "\n",
    "        # Capture all recipes on this page\n",
    "        recipes_json = json.loads(\n",
    "            response.xpath(\"//script[@id='allrecipes-schema_1-0']/text()\").get()\n",
    "        )\n",
    "\n",
    "        # Urls found in json\n",
    "        for recipe_url in recipes_json[0][\"itemListElement\"]:\n",
    "            # Get type of webpage linked (recipe, gallery, article)\n",
    "            # Only recipe pages have the data we want\n",
    "            if url.urlparse(recipe_url[\"url\"]).path.split(\"/\")[1] == \"recipe\":\n",
    "                yield scrapy.Request(recipe_url[\"url\"], self.parse_recipe)\n",
    "\n",
    "    def parse_recipe(self, response):\n",
    "        recipe = json.loads(\n",
    "            response.xpath(\"//script[@id='allrecipes-schema_1-0']/text()\").get()\n",
    "        )[0]\n",
    "\n",
    "        # Recipe information found in json\n",
    "        ingredients = recipe[\"recipeIngredient\"]\n",
    "        instructions = recipe[\"recipeInstructions\"]\n",
    "\n",
    "        def get_possibly_missing_key(data, key, default=0):\n",
    "            if key in data.keys():\n",
    "                val = data[key]\n",
    "            else:\n",
    "                val = default\n",
    "            return val\n",
    "\n",
    "        prep_time = get_possibly_missing_key(recipe, \"prepTime\", default=0)\n",
    "        cook_time = get_possibly_missing_key(recipe, \"cookTime\", default=0)\n",
    "        total_time = get_possibly_missing_key(recipe, \"totalTime\", default=0)\n",
    "        yield {\n",
    "            \"recipe_name\": recipe[\"name\"],\n",
    "            \"time\": {\n",
    "                \"prep\": prep_time,\n",
    "                \"cook\": cook_time,\n",
    "                \"total\": total_time,\n",
    "            },\n",
    "            \"ingredients\": ingredients,\n",
    "            \"instructions\": instructions,\n",
    "            \"body\": json.dumps(recipe),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running scrapy crawlers\n",
    "Scrapy crawlers can be run through the command line or using an external Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to main directory for running Scrapy spiders (./crawlers from top-level)\n",
    "!cd ./crawlers\n",
    "\n",
    "# allrecipes.com contains ~35k recipes. Running this command will generate approximately 700 MB of text data.\n",
    "!scrapy crawl allrecipes -o ../datasets/raw/test.jl -L INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The data gathered from the allrecipes.com crawler is stored in ./datasets/raw/allrecipes.jl. The JSON entries are encoded with UTF-8.\n",
    "A smaller subset of 100 recipes can be found in ./datasets/raw/sample.jl with the same format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mec-scrapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
